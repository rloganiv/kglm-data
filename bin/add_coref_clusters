#!/usr/bin/env python
from typing import Any, Tuple
import argparse
import json
import logging
from multiprocessing import JoinableQueue, Lock, Process, current_process, cpu_count
from pathlib import Path
from time import time

from kglm_data.coreference import CoreferenceResolver
from kglm_data.util import generate_instances, LOG_FORMAT

logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
print_lock = Lock()


def worker(q: JoinableQueue, output, FLAGS: Tuple[Any]):
    resolver = CoreferenceResolver(FLAGS.language, str(FLAGS.spacy_model_dir))

    while True:
        t0 = time()
        logger.debug(f'{current_process().name} taking a task from the queue')
        json_data = q.get()
        if json_data is None:
            break
        out = resolver.predict_json(json_data)
        logger.debug(f"Found {len(out['clusters'])} Coreference clusters for {out['title']}")
        print_lock.acquire()
        output.write(json.dumps(out) + '\n')
        print_lock.release()
        q.task_done()
        t1 = time()
        logger.debug(f'{current_process().name} finished {json_data["title"]} in {t1-t0} seconds')


def loader(q: JoinableQueue, FLAGS: Tuple[Any]) -> None:
    i = 0
    for i, instance in enumerate(generate_instances(FLAGS.input)):
        try:
            q.put(instance)
        except:
            logger.warning(f"Unable to load {instance} into queue")
    logger.info('All %i lines have been loaded into the queue', i+1)


def main(_):
    logger.info('Starting queue loader')
    work_queue = JoinableQueue(maxsize=256)
    loader_process = Process(target=loader, args=(work_queue, FLAGS))
    loader_process.start()

    num_processes = FLAGS.j
    if num_processes < 1 or num_processes > cpu_count():
        logger.debug(f"Setting num_processes to {cpu_count()} instead of {num_processes}")
        num_processes = cpu_count()
    logger.info(f'Launching {num_processes} worker processes')
    logger.info(f"Opening {FLAGS.output}")
    output = open(FLAGS.output, 'w')
    processes = [Process(target=worker, args=(work_queue, output, FLAGS)) for _ in range(num_processes)]
    for p in processes:
        p.start()

    loader_process.join()
    work_queue.join()
    for _ in range(num_processes):
        work_queue.put(None)
    for p in processes:
        p.join()
    logger.info('Done')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("input", type=Path)
    parser.add_argument('output', type=str)
    parser.add_argument('-j', type=int, default=1,
                        help='Number of processors')
    parser.add_argument('--debug', action='store_true')
    parser.add_argument("--language", type=str, default="sv")
    parser.add_argument("--spacy_model_dir", type=Path)
    FLAGS, _ = parser.parse_known_args()

    if FLAGS.debug:
        LEVEL = logging.DEBUG
    else:
        LEVEL = logging.INFO
    logging.basicConfig(format=LOG_FORMAT, level=LEVEL)

    main(_)


