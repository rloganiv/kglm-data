#!/usr/bin/env python

import json
import logging
from typing import List, Dict
from pathlib import Path

import spacy
from spacy.language import Language
from spacy.tokens.span import Span
from tqdm import tqdm

from kglm_data.util import LOG_FORMAT

logger = logging.getLogger(__name__)  # pylint: disable=invalid-name


def main(_):
    logger.info(f"Loading model from {FLAGS.spacy_model_dir}")
    nlp = spacy.load(FLAGS.spacy_model_dir)
    logger.info(f"Loaded model from {FLAGS.spacy_model_dir}")
    nlp.tokenizer = nlp.tokenizer.tokens_from_list

    with FLAGS.input.open() as f:
        for i, line in enumerate(f):
            pass
        num_docs = i + 1

    with FLAGS.input.open() as f, FLAGS.output.open("w") as fw:
        for line in tqdm(f, total=num_docs):
            doc = json.loads(line)
            doc["nel"] = get_nel(doc, nlp)
            fw.write(json.dumps(doc, ensure_ascii=False) + "\n")


def get_nel(doc: Dict, nlp: Language) -> List[Dict]:
    nel = []
    ents = nlp(doc["tokens"]).ents
    for ent in ents:
        if ent.kb_id_ != "NIL" and ent.kb_id_ != "":
            nel.append(get_nel_from_ent(ent))
    return nel


def get_nel_from_ent(ent: Span) -> Dict:
    nel = {}
    nel["tokens"] = ent.text
    nel["start"] = ent.start
    nel["end"] = ent.end
    nel["label"] = ent.kb_id_
    nel["score"] = 1.0
    return nel


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("input", type=Path)
    parser.add_argument('output', type=Path)
    parser.add_argument('--debug', action='store_true')
    parser.add_argument("--spacy_model_dir", required=True, type=Path)
    FLAGS, _ = parser.parse_known_args()

    if FLAGS.debug:
        LEVEL = logging.DEBUG
    else:
        LEVEL = logging.INFO
    logging.basicConfig(format=LOG_FORMAT, level=LEVEL)

    main(_)
